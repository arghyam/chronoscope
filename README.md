# Arghyam-BFM-Reading
Bulk Flow Meter reading extraction for Arghyam

# Stage 1: Data Cleaning

The first phase of the project is about data cleaning and removing duplicate images. This stage consists of two main steps:

## 1. Duplicate Image Removal
Script: `src/data_cleaning/0_remove_duplicate_images.py`

This script handles the removal of duplicate images from the dataset. It:
- Identifies and removes duplicate images that have the same base name with numbering (e.g., image.jpg, image(1).jpg)
- Keeps only the original version of each image
- Copies cleaned images to a new directory

Usage:
```bash
python src/data_cleaning/0_remove_duplicate_images.py
```

Configuration:
- Source folder: `data/original_images` - Directory containing the original dataset
- Destination folder: `data/refined_data` - Directory where cleaned images will be stored

The script will output:
- Original number of images
- Number of images after removing duplicates
- Number of duplicates removed

## 2. Image Quality Assessment
Script: `src/data_cleaning/1_streamlit_data_quality.py`

This interactive Streamlit application allows for manual quality assessment of the cleaned images. It helps identify and annotate images based on their quality characteristics.

Features:
- Interactive web interface for image annotation
- Six quality categories: Good, Blurry, Out of focus, Oriented, Foggy, Poor lighting
- Automatic progress saving
- Resume capability for interrupted annotation sessions

Usage:
```bash
streamlit run src/data_cleaning/1_streamlit_data_quality.py
```

The tool:
- Displays images one at a time
- Allows quick annotation through button clicks
- Saves annotations automatically to a CSV file (`annotations.csv`) in the image folder
- Tracks progress and allows resuming from where you left off

Output:
- Creates an `annotations.csv` file with columns:
  - image_name: Name of the image file
  - annotation: Selected quality category
  - annotation_done: Annotation status

## 3. Data Distribution Analysis
Script: `src/data_cleaning/2_data_distribution.py`

This script analyzes the distribution of image annotations from the quality assessment process. It provides a statistical overview of the dataset quality.

Features:
- Reads the annotations CSV file generated by the quality assessment tool
- Calculates the distribution of images across different quality categories
- Displays the count and percentage for each annotation category

Usage:
```bash
python src/data_cleaning/2_data_distribution.py
```

Output:
- Prints a summary of annotation distribution to the console
- Shows both count and percentage for each quality category

## 4. Perceptual Duplicate Detection
Script: `src/data_cleaning/4_removing_duplicate_data_same_images.py`

This script identifies and removes visually duplicate images using perceptual hashing techniques. Unlike the first duplicate removal script, this one detects images that look similar even if they have different filenames.

Features:
- Multiple hash methods available: perceptual hash (phash), difference hash (dhash), average hash (ahash), and MD5
- Identifies visually similar images regardless of filename
- Preserves only one copy of each unique image

Usage:
```bash
python src/data_cleaning/4_removing_duplicate_data_same_images.py
```

Configuration:
- Input folder: `data/original_images_2` - Directory containing images to check for duplicates
- Output folder: `data/refined_data_2` - Directory where unique images will be stored
- Hash method: 'phash' (default) - Perceptual hash algorithm for image comparison

The script will output:
- Total number of images processed
- Number of unique images identified and copied
- Number of duplicate images skipped

## 5. Image Orientation Detection
Script: `src/data_cleaning/5_checking_orientation_images.py`

This script analyzes images to determine if they are properly oriented or rotated. It uses multiple methods including EXIF metadata and OCR-based detection to identify images that may need rotation.

Features:
- EXIF metadata analysis to check for orientation flags
- OCR-based orientation detection using text recognition
- Multiple detection methods with fallback options
- Comprehensive CSV report generation

Usage:
```bash
python src/data_cleaning/5_checking_orientation_images.py --input_dir [input_directory] --output_csv [output_file.csv]
```

Parameters:
- `--input_dir`: Directory containing images to check for orientation
- `--output_csv`: Path to save the CSV report

The script will output:
- A CSV file with columns:
  - image_name: Name of the image file
  - is_properly_oriented: 'Yes' or 'No' indicating orientation status
- Progress updates during processing

This comprehensive data cleaning pipeline ensures that the dataset is free from duplicates, contains only high-quality images, and provides insights into the distribution and orientation of the images for further processing.


